import logging
import joblib
import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer
from pathlib import Path
from typing import Dict, Tuple, Union

logger = logging.getLogger("data.DataLoader")

class CustomDataset(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œå¤„ç†æ•°æ®åŠ è½½å’Œé¢„å¤„ç†"""
    
    def __init__(
        self, 
        data: pd.DataFrame,
        tokenizer: BertTokenizer,
        mode: str,
        max_length: int = 128,
        seed: int = 42,
        cache_dir: str = "data/processed/data_cache",
        ):
        self.data = data
        self.mode = mode
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.seed = seed
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.label_prefixes = ['Professionalism', 'Occupational', 'Effectiveness', 'Quality', 'Other']
        self.target_columns = [f"{prefix}_score" for prefix in self.label_prefixes]
        
        self._validate_data()
        self._preprocess_and_cache()
        
    def _validate_data(self):
        """éªŒè¯æ•°æ®é›†ä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ—"""
        required_columns = ['comments'] + \
                          [f"{prefix}_reason" for prefix in self.label_prefixes] + \
                          self.target_columns
        missing_cols = [col for col in required_columns if col not in self.data.columns]
        if missing_cols:
            raise ValueError(f"æ•°æ®é›†ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
        
    def _preprocess_and_cache(self):
        """é¢„å¤„ç†æ•°æ®å¹¶ç¼“å­˜ç»“æœ"""
        cache_file = self.cache_dir / f"processed_{self.mode}_{self.seed}_{len(self.data)}_{self.max_length}.pkl"
        if cache_file.exists():
            self.cached_data = joblib.load(cache_file)
            logger.info(f"ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†æ•°æ®: {cache_file}")
            return
            
        self.cached_data = []
        for idx in range(len(self.data)):
            try:
                text = str(self.data.iloc[idx]['comments'])
                text_encoding = self.tokenizer(
                    text, 
                    max_length=self.max_length,
                    padding='max_length', 
                    truncation=True,
                    return_tensors='pt'
                )
                
                prompts = []
                for prefix in self.label_prefixes:
                    reason_col = f"{prefix}_reason"
                    prompt = str(self.data.iloc[idx][reason_col])
                    prompts.append(prompt)
                
                target_scores = self.data[self.target_columns].iloc[idx].values
                if not all(0 <= score <= 5 for score in target_scores):
                    raise ValueError(f"ç´¢å¼• {idx} çš„ç›®æ ‡åˆ†æ•°è¶…å‡º0-5èŒƒå›´: {target_scores}")
                target_tensor = torch.tensor(target_scores, dtype=torch.float)
                
                self.cached_data.append({
                    'input_ids': text_encoding['input_ids'].squeeze(0),
                    'attention_mask': text_encoding['attention_mask'].squeeze(0),
                    'prompts': prompts,
                    'target': target_tensor
                })
                
            except Exception as e:
                logger.error(f"å¤„ç†ç´¢å¼• {idx} æ—¶å‡ºé”™: {str(e)}")
                continue  # è·³è¿‡é—®é¢˜æ ·æœ¬
                
        joblib.dump(self.cached_data, cache_file)
        logger.info(f"é¢„å¤„ç†æ•°æ®å·²ç¼“å­˜: {cache_file}")
    
    def __len__(self):
        return len(self.cached_data)
    
    def __getitem__(self, idx):
        return self.cached_data[idx]

class DataLoaderModule:
    """æ•°æ®åŠ è½½æ¨¡å—"""
    
    def __init__(self, config: Dict[str, Union[str, int, float]], tokenizer: BertTokenizer):
        self.config = config
        self.tokenizer = tokenizer
        self._load_data()
        self._split_datasets()
        
    def _load_data(self) -> None:
        logger.info(f"ä» {self.config['csv_path']} åŠ è½½æ•°æ®")
        try:
            self.full_data = pd.read_csv(self.config['csv_path'])
            if self.full_data.isnull().sum().sum() > 0:
                logger.warning("æ•°æ®åŒ…å«ç¼ºå¤±å€¼ï¼Œå¯èƒ½ä¼šå½±å“æ¨¡å‹è®­ç»ƒ")
            logger.info(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œæ€»æ ·æœ¬æ•°: {len(self.full_data)}")
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            raise
            
    def _split_datasets(self) -> None:
        logger.info("åˆ’åˆ†æ•°æ®é›†...")
        np.random.seed(self.config['random_seed'])
        torch.manual_seed(self.config['random_seed'])
        
        if abs(self.config['train_ratio'] + self.config['val_ratio'] + self.config['test_ratio'] - 1.0) > 1e-6:
            raise ValueError("è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1.0")
        
        total_size = len(self.full_data)
        train_size = int(total_size * self.config['train_ratio'])
        val_size = int(total_size * self.config['val_ratio'])
        test_size = total_size - train_size - val_size
        
        if test_size < 0:
            raise ValueError("åˆ’åˆ†æ¯”ä¾‹æ€»å’Œä¸èƒ½è¶…è¿‡1.0")
        
        indices = np.arange(total_size)
        train_indices, temp_indices = train_test_split(
            indices, 
            test_size =1 - self.config['train_ratio'],
            random_state = self.config['random_seed']
        )
        
        val_test_ratio = self.config['test_ratio'] / (self.config['val_ratio'] + self.config['test_ratio'])
        val_indices, test_indices = train_test_split(
            temp_indices,
            test_size = val_test_ratio,
            random_state = self.config['random_seed']
        )
        
        self.train_dataset = CustomDataset(
            self.full_data.iloc[train_indices].reset_index(drop=True),
            self.tokenizer,
            'TRAIN',
            self.config['max_length'],
            self.config['random_seed'],
        )
        
        self.val_dataset = CustomDataset(
            self.full_data.iloc[val_indices].reset_index(drop=True),
            self.tokenizer,
            'VAL',
            self.config['max_length'],
            self.config['random_seed'],

        )
        
        self.test_dataset = CustomDataset(
            self.full_data.iloc[test_indices].reset_index(drop=True),
            self.tokenizer,
            'TEST',
            self.config['max_length'],
            self.config['random_seed'],
        )
        
        logger.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›†={len(self.train_dataset)}, "
                    f"éªŒè¯é›†={len(self.val_dataset)}, æµ‹è¯•é›†={len(self.test_dataset)}")
        
    def get_dataloaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:
        train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=self.config['num_workers'],
            pin_memory=True,
            drop_last=True
        )
        
        val_loader = DataLoader(
            self.val_dataset,
            batch_size=min(self.config['batch_size'], len(self.val_dataset)),
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        test_loader = DataLoader(
            self.test_dataset,
            batch_size=min(self.config['batch_size'], len(self.test_dataset)),
            shuffle=False,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        return train_loader, val_loader, test_loader
    
if __name__ == '__main__':
    import os
    tokenizer = BertTokenizer.from_pretrained("models/save/tokenizer")
    CONFIG = {
        'csv_path': 'data/processed/data.csv',
        'max_length': 128,
        'batch_size': 64,
        'train_ratio': 0.7,
        'val_ratio': 0.15,
        'test_ratio': 0.15,
        'random_seed': 42,
        'num_workers': min(4, os.cpu_count() - 1),
    }
    data_module = DataLoaderModule(CONFIG, tokenizer)
    train_loader, val_loader, test_loader = data_module.get_dataloaders()
    
    def inspect_data_loader(loader, loader_name, max_batches=200):
        print(f"ğŸ” å¼€å§‹æ£€æŸ¥ {loader_name} æ•°æ®åŠ è½½å™¨ï¼Œæœ€å¤šæ£€æŸ¥ {max_batches} ä¸ªæ‰¹æ¬¡...")
        
        # å­˜å‚¨å‘ç°çš„é—®é¢˜
        problems_found = []
        
        for batch_idx, batch in enumerate(loader):
            if batch_idx >= max_batches:
                break
            print(f"\n=== æ‰¹æ¬¡ {batch_idx} ===")
            
            # 1. æ£€æŸ¥æ¯ä¸ªå­—æ®µæ˜¯å¦å­˜åœ¨NaN/Inf
            for key in ['input_ids', 'attention_mask', 'target']:
                data = batch[key]
                
                # æ£€æŸ¥NaN
                nan_mask = torch.isnan(data)
                if nan_mask.any():
                    nan_count = nan_mask.sum().item()
                    problems_found.append(f"{key} åŒ…å«NaNå€¼ (æ•°é‡: {nan_count})")
                    print(f"âŒ é—®é¢˜: {key} åŒ…å«NaNå€¼ (æ•°é‡: {nan_count})")
                
                # æ£€æŸ¥æ— ç©·å€¼
                inf_mask = torch.isinf(data)
                if inf_mask.any():
                    inf_count = inf_mask.sum().item()
                    problems_found.append(f"{key} åŒ…å«æ— ç©·å€¼ (æ•°é‡: {inf_count})")
                    print(f"âŒ é—®é¢˜: {key} åŒ…å«æ— ç©·å€¼ (æ•°é‡: {inf_count})")
            
            # 2. æ£€æŸ¥input_idsçš„èŒƒå›´
            input_ids = batch['input_ids']
            min_id = input_ids.min().item()
            max_id = input_ids.max().item()
            print(f"input_ids èŒƒå›´: {min_id} åˆ° {max_id}")
            
            if min_id < 0:
                problems_found.append(f"input_ids åŒ…å«è´Ÿå€¼ (æœ€å°å€¼: {min_id})")
                print(f"âŒ é—®é¢˜: input_ids åŒ…å«è´Ÿå€¼ (æœ€å°å€¼: {min_id})")
            
            # 3. æ£€æŸ¥attention_maskçš„å€¼
            attn_mask = batch['attention_mask']
            unique_vals = torch.unique(attn_mask).tolist()
            print(f"attention_mask å”¯ä¸€å€¼: {unique_vals}")
            
            if set(unique_vals) - {0, 1}:
                problems_found.append(f"attention_mask åŒ…å«éæ³•å€¼: {unique_vals}")
                print(f"âŒ é—®é¢˜: attention_mask åŒ…å«éæ³•å€¼: {unique_vals}")
            
            # 4. æ£€æŸ¥targetæ ‡ç­¾
            targets = batch['target']
            
            # æ£€æŸ¥å¤šæ ‡ç­¾åˆ†ç±»çš„targetå€¼
            if targets.dim() > 1:  # å¤šæ ‡ç­¾æƒ…å†µ
                min_val = targets.min().item()
                max_val = targets.max().item()
                print(f"target å€¼èŒƒå›´: {min_val} åˆ° {max_val}")
                
                if min_val < 0 or max_val > 5:
                    problems_found.append(f"target å€¼è¶…å‡º[0,1]èŒƒå›´: [{min_val}, {max_val}]")
                    print(f"âŒ é—®é¢˜: target å€¼è¶…å‡º[0,1]èŒƒå›´: [{min_val}, {max_val}]")
            
            # 5. æ£€æŸ¥æ•°æ®å½¢çŠ¶ä¸€è‡´æ€§
            batch_size = input_ids.size(0)
            for key in ['input_ids', 'attention_mask', 'target']:
                if batch[key].size(0) != batch_size:
                    problems_found.append(f"æ‰¹æ¬¡ä¸­æ ·æœ¬æ•°é‡ä¸ä¸€è‡´: {key}")
                    print(f"âŒ é—®é¢˜: æ‰¹æ¬¡ä¸­æ ·æœ¬æ•°é‡ä¸ä¸€è‡´: {key}")
            
            # 6. æ‰“å°æ ·æœ¬ä¿¡æ¯
            print("\næ ·æœ¬ç¤ºä¾‹:")
            for i in range(min(2, batch_size)):  # æ‰“å°å‰2ä¸ªæ ·æœ¬
                sample = {k: v[i] for k, v in batch.items()}
                
                # è·å–å®é™…æ–‡æœ¬ï¼ˆå¦‚æœæœ‰tokenizerï¼‰
                text = f"input_ids: {sample['input_ids'][:10]}..."  # åªæ˜¾ç¤ºå‰10ä¸ªtoken
                try:
                    text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)
                except:
                    pass
                
                print(f"æ ·æœ¬ {i}:")
                print(f"  æ–‡æœ¬: {text[:100]}...")  # æˆªæ–­é•¿æ–‡æœ¬
                print(f"  attention_mask: {sample['attention_mask'][:10]}...")
                print(f"  target: {sample['target']}")
                print(f"  prompt: {sample['prompts'][i][:50]}...")  # æ˜¾ç¤ºéƒ¨åˆ†prompt
                
        # æœ€ç»ˆæŠ¥å‘Š
        if not problems_found:
            print(f"\nâœ… {loader_name} æ•°æ®æ£€æŸ¥å®Œæˆï¼Œæœªå‘ç°æ˜æ˜¾é—®é¢˜")
        else:
            print(f"\nâ›” {loader_name} å‘ç°é—®é¢˜æ±‡æ€»:")
            for problem in problems_found:
                print(f"  - {problem}")
    
    # # æ£€æŸ¥è®­ç»ƒé›†
    # inspect_data_loader(train_loader, "è®­ç»ƒé›†")
    # # æ£€æŸ¥éªŒè¯é›†
    # inspect_data_loader(val_loader, "éªŒè¯é›†")
    # # æ£€æŸ¥æµ‹è¯•é›†
    # inspect_data_loader(test_loader, "æµ‹è¯•é›†")